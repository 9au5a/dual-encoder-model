project: dual-encoder
name: dual-encoder-config

hparam:
  seed: 0
  device_id: [0,1]
  batch_size: 1
  problem_type: regression
  num_labels: 1 

  transformer_pretrained_id: sentence-transformers/all-mpnet-base-v2
  st_max_token_length: 256 

  prefix_max_token_length: 128
  pre_seq_len: 10

  prefix_backbone: 'roberta'

  fusion_method: avg 
  encoding_projection_size: 128
  
  lstm_input_size: 128
  lstm_hidden_size: 64
  lstm_num_layers: 1
  
  dropout_prob: 0.5

  optimizer: AdamW
  learning_rate: 3.e-4
  num_epoch: 200
  es_patience: 20
  

  


